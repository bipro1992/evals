
from strands import Agent
from strands_evaluation.dataset import Dataset 
from strands_evaluation.case import Case
from strands_evaluation.evaluators.evaluator import Evaluator
from strands_evaluation.types.evaluation import EvaluationOutput, EvaluationData

## Using a third party evaluator
from langchain_aws import BedrockLLM
from langchain.evaluation.criteria import CriteriaEvalChain

def third_party_example():
    ### Step 1: Create test cases ###
    test_case1 = Case[str, str](name = "knowledge-1",
                                    input="What is the capital of France?",
                                    expected_output="The capital of France is Paris.",
                                    metadata={"category": "knowledge"})
                                    
    test_case2 = Case[str, str](name = "knowledge-2",
                                    input="What color is the ocean?",
                                    expected_output="The ocean is blue.",
                                    metadata={"category": "knowledge"})

    ### Step 2: Create evaluator using a third party evaluator ###                                                        
    class TestSimilarityEvaluator(Evaluator[str, str]):
        def evaluate(self, evaluation_case: EvaluationData[str, str]) -> EvaluationOutput:

            ## Follow LangChain's Docs: https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html
            # Initialize Bedrock LLM
            bedrock_llm = BedrockLLM(
                model_id="anthropic.claude-v2",  # or other Bedrock models
                model_kwargs={
                    "max_tokens_to_sample": 256,
                    "temperature": 0.7,
                }
            )

            criteria = {
                "correctness": "Is the actual answer correct?",
                "relevance": "Is the response relevant?"
            }
            
            evaluator = CriteriaEvalChain.from_llm(
                llm=bedrock_llm,
                criteria=criteria
            )
            
            # Pass in required context for evaluator (look at LangChain's docs)
            result = evaluator.evaluate_strings(
                prediction=evaluation_case.actual_output,
                input=evaluation_case.input
            )
            
            # Make sure to return the correct type
            return EvaluationOutput(score=result["score"], test_pass= True if result["score"] > .5 else False, reason = result["reasoning"])
               
            
    ### Step 3: Create dataset ###                                    
    dataset = Dataset[str, str](cases = [test_case1, test_case2],
                                evaluator = TestSimilarityEvaluator())
    
    ### Step 4: Define task ###  
    def get_response(query: str) -> str:
        agent = Agent(callback_handler=None)
        return str(agent(query))

    ### Step 5: Run evaluation ###                            
    report = dataset.run_evaluations(get_response)
    return report



if __name__ == "__main__":
    # run the file as a module: eg. python -m examples.third_party_evaluator 
    report = third_party_example()
    # print(report)
    report.display(include_input=False)
